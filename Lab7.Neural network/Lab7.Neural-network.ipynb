{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB7 tutorial for Machine Learning <br >Neural NetWork & Pytorch\n",
    "> The document description are designed by JIa Yanhong in 2022. Oct. 20th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Install the pytorch neural network framework for your computer.\n",
    "- Learn to use pytorch.\n",
    "- Implement a simple neural network using pytorch\n",
    "- Complete the LAB assignment and submit it to BB.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of  machine learning and are at the heart of  deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "\n",
    "![img](images/1hkYlTODpjJgo32DoCOWN5w.png)\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing `an input layer`, `one or more hidden layers`, and `an output layer`. Each node, or artificial neuron, connects to another and has an associated `weight` and `threshold`. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "<img src=\"images/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png\" alt=\"Visual diagram of an input layer, hidden layers, and an output layer of a feedforward neural network \" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The role of neural networks in Machine learning\n",
    "+ Supervise machine learning process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Supervise machine learning process ](images/Supervise-machine-learning-process.png)\n",
    "\n",
    "  In the figure above, the hardest part is how to obtain valid feature vectors. This is a technique called **feature engineering**.\n",
    "\n",
    "+ **The Importance of Feature Engineering:**\n",
    "\n",
    "  + Preprocessing and feature extraction determine the upper bound of the model\n",
    "  + The algorithm and parameter selection approach this upper bound.\n",
    "\n",
    "+ **Traditional feature extraction methods:**\n",
    "\n",
    "  <img src=\"images/image-20221020212446504.png\" alt=\"image-20221020212446504 \" style=\"zoom:60%;\" />\n",
    "\n",
    "+ **Neural networks automatically extract features**\n",
    "  \n",
    "  <img src=\"images/image-20221020212805853.png\" alt=\"image-20221020212805853 \" style=\"zoom:60%;\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "### Install Pytorch\n",
    "Please refer to **Installing PyTorch on Windows 10.md** for this section:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning pytorch with linear regression\n",
    "#### Linear regression\n",
    "Do you remember the general form of linear regression model's prediction?\n",
    "$$\\hat{y}=h_{\\theta }(x)=\\theta _{0}+\\theta _{1}x_{1}+\\theta _{2}x_{2}+...+\\theta _{n}x_{n}=\\theta ^{T}\\cdot x$$\n",
    "\n",
    "<center>\n",
    "    <img src='images/fit-linreg.svg' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "       Fitting a linear regression model to one-dimensional data\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "Linear regression is a single-layer neural network, We used this simple network to learn how to use pytorch.\n",
    "<center>\n",
    "    <img src='images/singleneuron.svg' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "        Linear regression is a single-layer neural network \n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warm-up: [numpy](https://numpy.org/learn/)\n",
    "Before introducing PyTorch, we will first implement the network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a third order polynomial to sine function by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x221ec5c4c40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.scatter(x,y,c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 5003.872443721875\n",
      "199 3353.270888060061\n",
      "299 2249.688026436949\n",
      "399 1511.3632122032495\n",
      "499 1017.0738166883739\n",
      "599 685.9280104550094\n",
      "699 463.91710389639684\n",
      "799 314.9606030575069\n",
      "899 214.94039773340526\n",
      "999 147.72448048355977\n",
      "1099 102.51541919362238\n",
      "1199 72.0813164766731\n",
      "1299 51.574908662489676\n",
      "1399 37.74482385992415\n",
      "1499 28.408456323926398\n",
      "1599 22.099462991993885\n",
      "1699 17.8318812608334\n",
      "1799 14.942178006552481\n",
      "1899 12.983407693060862\n",
      "1999 11.654235967970715\n",
      "Result: y = -0.037462544060796896 + 0.8180468799142779 x + 0.006462912229911405 x^2 + -0.08782650973115476 x^3\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: Tensors \n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1055.6630859375\n",
      "199 707.0202026367188\n",
      "299 474.7330627441406\n",
      "399 319.8968505859375\n",
      "499 216.63632202148438\n",
      "599 147.73626708984375\n",
      "699 101.73805236816406\n",
      "799 71.01164245605469\n",
      "899 50.4744873046875\n",
      "999 36.73887252807617\n",
      "1099 27.546342849731445\n",
      "1199 21.390058517456055\n",
      "1299 17.264141082763672\n",
      "1399 14.496957778930664\n",
      "1499 12.639598846435547\n",
      "1599 11.39195728302002\n",
      "1699 10.553153991699219\n",
      "1799 9.988704681396484\n",
      "1899 9.608565330505371\n",
      "1999 9.352319717407227\n",
      "Result: y = 0.014220132492482662 + 0.8384338021278381 x + -0.0024532105308026075 x^2 + -0.09072636812925339 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: Tensors and autograd\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. \n",
    "\n",
    "When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If `x` is a Tensor that has `x.requires_grad=True` then `x.grad` is another Tensor holding the gradient of `x` with respect to some scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1040.6602783203125\n",
      "199 702.6412963867188\n",
      "299 475.760009765625\n",
      "399 323.3516540527344\n",
      "499 220.88526916503906\n",
      "599 151.93601989746094\n",
      "699 105.49869537353516\n",
      "799 74.19435119628906\n",
      "899 53.0714225769043\n",
      "999 38.80476760864258\n",
      "1099 29.159080505371094\n",
      "1199 22.63106346130371\n",
      "1299 18.208301544189453\n",
      "1399 15.208748817443848\n",
      "1499 13.172165870666504\n",
      "1599 11.787906646728516\n",
      "1699 10.845947265625\n",
      "1799 10.20425796508789\n",
      "1899 9.766622543334961\n",
      "1999 9.467809677124023\n",
      "Result: y = 0.019743070006370544 + 0.8398270010948181 x + -0.003406008007004857 x^2 + -0.09092454612255096 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from importlib.metadata import requires\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype,requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype,requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype,requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype,requires_grad=True)\n",
    "# a = V(a,requires_grad=True)\n",
    "# #a.requires_grad = True\n",
    "# b.requires_grad = True\n",
    "# c.requires_grad = True\n",
    "# d.requires_grad = True\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    #loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.data.item())\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    # grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # grad_a = grad_y_pred.sum()\n",
    "    # grad_b = (grad_y_pred * x).sum()\n",
    "    # grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    # grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a.data -= learning_rate * a.grad.data\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "    c.data -= learning_rate * c.grad.data\n",
    "    d.data -= learning_rate * d.grad.data\n",
    "\n",
    "    a.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    c.grad.data.zero_()\n",
    "    d.grad.data.zero_()\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.data.item()} + {b.data.item()} x + {c.data.item()} x^2 + {d.data.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch:[nn module](https://pytorch.org/docs/stable/nn.html)\n",
    "we use the nn package to implement our polynomial model network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Using torch.nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 168.69937133789062\n",
      "199 115.20408630371094\n",
      "299 79.63618469238281\n",
      "399 55.9792366027832\n",
      "499 40.238746643066406\n",
      "599 29.761474609375\n",
      "699 22.784587860107422\n",
      "799 18.136669158935547\n",
      "899 15.038793563842773\n",
      "999 12.973041534423828\n",
      "1099 11.594830513000488\n",
      "1199 10.67483901977539\n",
      "1299 10.060365676879883\n",
      "1399 9.649728775024414\n",
      "1499 9.375120162963867\n",
      "1599 9.19136905670166\n",
      "1699 9.068315505981445\n",
      "1799 8.985870361328125\n",
      "1899 8.930582046508789\n",
      "1999 8.893481254577637\n",
      "Result: y = 0.00474360678344965 + 0.8494515419006348 x + -0.0008183512254618108 x^2 + -0.09229354560375214 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define the class\n",
    "  \n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in __init__. Every `nn.Module` subclass implements the operations on input data in the `forward()` method.\n",
    "The `forward()` method is in charge of conducting the **forward propagation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch import nn\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()       \n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.flatten = nn.Flatten(0, 1)\n",
    "#       self.model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(3, 1),\n",
    "#     torch.nn.Flatten(0, 1)\n",
    "# )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.flatten(self.linear(x))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 560.5450439453125\n",
      "199 377.87451171875\n",
      "299 255.84808349609375\n",
      "399 174.2834930419922\n",
      "499 119.72988891601562\n",
      "599 83.2184066772461\n",
      "699 58.765220642089844\n",
      "799 42.376243591308594\n",
      "899 31.383811950683594\n",
      "999 24.005237579345703\n",
      "1099 19.048444747924805\n",
      "1199 15.71574592590332\n",
      "1299 13.473033905029297\n",
      "1399 11.962526321411133\n",
      "1499 10.944223403930664\n",
      "1599 10.257067680358887\n",
      "1699 9.792941093444824\n",
      "1799 9.479118347167969\n",
      "1899 9.266721725463867\n",
      "1999 9.122815132141113\n",
      "Result: y = -0.011960328556597233 + 0.8437725901603699 x + 0.0020633554086089134 x^2 + -0.09148576855659485 x^3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model.linear\n",
    "\n",
    "\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: optim\n",
    "Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters with `torch.no_grad()`. This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc.\n",
    "\n",
    "The `optim` package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms.\n",
    "\n",
    "In this example we will use the `nn` package to define our model as before, but we will optimize the model using the `RMSprop` algorithm provided by the `optim` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 7999.2177734375\n",
      "199 3253.723388671875\n",
      "299 1712.0457763671875\n",
      "399 1069.9207763671875\n",
      "499 769.9473266601562\n",
      "599 602.9730224609375\n",
      "699 469.6629943847656\n",
      "799 353.4874572753906\n",
      "899 255.88104248046875\n",
      "999 177.02703857421875\n",
      "1099 115.75796508789062\n",
      "1199 70.49478149414062\n",
      "1299 39.61913299560547\n",
      "1399 21.222084045410156\n",
      "1499 12.302330017089844\n",
      "1599 9.382658004760742\n",
      "1699 8.970685958862305\n",
      "1799 9.071578025817871\n",
      "1899 8.891473770141602\n",
      "1999 8.89632511138916\n",
      "Result: y = 0.00023304199567064643 + 0.8563223481178284 x + 0.00023340372717939317 x^2 + -0.09380150586366653 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "model.requires_grad_()\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "optimizer = torch.optim.RMSprop(params=model.parameters(),lr=0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(),lr=1e-6,momentum=0.9)\n",
    "\n",
    "for t in range(2000):\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted y by passing x to the model. \n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "   \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())    \n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. \n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning pytorch with logistic regression\n",
    "If we use a single-layer network for classification, this is known as a logistic regression.\n",
    "\n",
    "\n",
    " We need to add the sigmoid function to the output of the linear regression.\n",
    "<center>\n",
    "    <img src='images/Center.png' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "       Perceptron\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "Let us define the number of epochs and the learning rate we want our model for training. As the data is a binary  classification, we will use **Binary Cross Entropy** as the **loss function** used to optimize the model using an `SGD optimizer`.\n",
    "\n",
    "<font size=5 color='red'>Please complete this part of the code!!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "### Perceptron-The basic unit of neural network\n",
    "A simple model of a biological neuron in an artificial neural network is known as Perceptron. it is the primary step to learn Machine Learning and Deep Learning technologies.\n",
    "\n",
    "we can consider it as a single-layer neural network with four main parameters, i.e., `input values`, `weights and Bias`, `net sum`, and an `activation function`.\n",
    "\n",
    "![Perceptron in Machine Learning](images/perceptron-in-machine-learning2.png)\n",
    "\n",
    "- **Input Nodes or Input Layer:**\n",
    "\n",
    "This is the primary component of Perceptron which accepts the initial data into the system for further processing.\n",
    "\n",
    "- **Wight and Bias:**\n",
    "\n",
    "Weight parameter represents the strength of the connection between units.  Bias can be considered as the intercept in a linear equation.\n",
    "\n",
    "- **Activation Function:**\n",
    "\n",
    "These are the final and important components that help to determine whether the neuron will fire or not. The activation function of perceptron is `sign function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron(MLP)\n",
    "A neuron is a mathematical model of the behaviour of a single neuron in a biological nervous system.\n",
    "\n",
    "A single neuron can solve some simple tasks, but the power of neural networks comes when many of them are arranged in layers and connected in a network architecture.\n",
    "\n",
    "<img src=\"images/multilayer-perceptron-1.png\" alt=\"multilayer-perceptron-1 \" style=\"zoom:40%;\" />\n",
    "\n",
    "\n",
    "**A Multilayered Perceptron is a Neural Network**. A neural network having more than 3 hidden layers is called a **Deep Neural Network**.\n",
    "\n",
    "In this lab, Multilayer Perceptron and Neural Network  mean the same thing.\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Various activation functions that can be used with Perceptron.\n",
    "\n",
    "![Perceptron_36.](images/Perceptron_36.jpg)\n",
    "\n",
    "<font color=\"red\">Neural network without activation functions are simply linear regression model</font>. The activation makes the input capable of learning and performing more complex tasks.\n",
    "\n",
    "![image-20221023014216170](images/image-20221023014216170.png)\n",
    "\n",
    "Therefore, when we write the neural network framework, the neurons in each hidden layer are most of the time **followed by an activation function**.\n",
    "\n",
    "I recommend that you use the relu function as you build your neural network framework.\n",
    "\n",
    "![image-20221023015025204](images/image-20221023015025204.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for MLP\n",
    "This Example uses dataset digit123.csv , which has 36 columns, and the last column is the dependent variable. We use this dataset to familiarize ourselves with MLP and solve the multi-classification problem.\n",
    "\n",
    "\n",
    "**Note that the values of the dependent variable are 1,2,3, and label coding is required.**\n",
    "#### MLP Model \n",
    "\n",
    "+ step 1 load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  27  28  29  30  31  32  33  \\\n",
       "0   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   0   1   \n",
       "1   0   0   0   1   0   0   0   0   1   1  ...   1   0   0   0   0   0   1   \n",
       "2   0   0   1   1   0   0   0   0   0   1  ...   1   0   0   0   0   0   1   \n",
       "3   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   1   1   \n",
       "4   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   1   1   \n",
       "\n",
       "   34  35  36  \n",
       "0   0   0   1  \n",
       "1   0   0   1  \n",
       "2   0   0   1  \n",
       "3   0   0   1  \n",
       "4   1   0   1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "# ============================ step 1/6 load datasets ============================\n",
    "df = pd.read_csv(\"datasets/digit123.csv\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    32\n",
       "2    32\n",
       "3    32\n",
       "Name: 36, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[36].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 36)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[36]\n",
    "y.replace((1, 2, 3),(0, 1, 2),inplace=True)\n",
    "X = df.drop(36, axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting for X and Y variables:\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Splitting dataset into 80% Training and 20% Testing Data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8, random_state =0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch \n",
    "#Converting them to tensors as PyTorch works on, we will use the torch.from_numpy() method:\n",
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "X_test = torch.from_numpy(X_test.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "y_test = torch.from_numpy(y_test.values).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 2 Define a MLP subclass of nn. Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a MLP subclass of nn. Module.\n",
    "# ============================ step 2/6 define model ============================\n",
    "import  torch \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_i, n_h, n_o):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(n_i, n_h)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(n_h, n_o)\n",
    "    def forward(self, input):\n",
    "        return self.linear2(self.relu(self.linear1(self.flatten(input))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 3 Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.001 \n",
    "# Create the model\n",
    "# ============================ step 3/6 Create model ============================\n",
    "models = MLP(X_train.shape[1],X_train.shape[1]//2,y_train.unique().size()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 4 Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 4/6 Loss function ============================\n",
    "criterions = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 5 The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 4/6 The optimizer ============================\n",
    "optimizers = torch.optim.SGD(models.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 6 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss = 1.1263\n",
      "epoch: 40, loss = 1.1233\n",
      "epoch: 60, loss = 1.1203\n",
      "epoch: 80, loss = 1.1174\n",
      "epoch: 100, loss = 1.1146\n",
      "epoch: 120, loss = 1.1119\n",
      "epoch: 140, loss = 1.1093\n",
      "epoch: 160, loss = 1.1067\n",
      "epoch: 180, loss = 1.1041\n",
      "epoch: 200, loss = 1.1016\n",
      "epoch: 220, loss = 1.0990\n",
      "epoch: 240, loss = 1.0965\n",
      "epoch: 260, loss = 1.0939\n",
      "epoch: 280, loss = 1.0914\n",
      "epoch: 300, loss = 1.0888\n",
      "epoch: 320, loss = 1.0863\n",
      "epoch: 340, loss = 1.0838\n",
      "epoch: 360, loss = 1.0814\n",
      "epoch: 380, loss = 1.0789\n",
      "epoch: 400, loss = 1.0765\n",
      "epoch: 420, loss = 1.0741\n",
      "epoch: 440, loss = 1.0717\n",
      "epoch: 460, loss = 1.0693\n",
      "epoch: 480, loss = 1.0670\n",
      "epoch: 500, loss = 1.0646\n",
      "epoch: 520, loss = 1.0622\n",
      "epoch: 540, loss = 1.0598\n",
      "epoch: 560, loss = 1.0575\n",
      "epoch: 580, loss = 1.0551\n",
      "epoch: 600, loss = 1.0528\n",
      "epoch: 620, loss = 1.0504\n",
      "epoch: 640, loss = 1.0481\n",
      "epoch: 660, loss = 1.0458\n",
      "epoch: 680, loss = 1.0434\n",
      "epoch: 700, loss = 1.0411\n",
      "epoch: 720, loss = 1.0387\n",
      "epoch: 740, loss = 1.0364\n",
      "epoch: 760, loss = 1.0341\n",
      "epoch: 780, loss = 1.0318\n",
      "epoch: 800, loss = 1.0295\n",
      "epoch: 820, loss = 1.0271\n",
      "epoch: 840, loss = 1.0248\n",
      "epoch: 860, loss = 1.0225\n",
      "epoch: 880, loss = 1.0201\n",
      "epoch: 900, loss = 1.0178\n",
      "epoch: 920, loss = 1.0154\n",
      "epoch: 940, loss = 1.0130\n",
      "epoch: 960, loss = 1.0106\n",
      "epoch: 980, loss = 1.0082\n",
      "epoch: 1000, loss = 1.0058\n",
      "epoch: 1020, loss = 1.0034\n",
      "epoch: 1040, loss = 1.0009\n",
      "epoch: 1060, loss = 0.9985\n",
      "epoch: 1080, loss = 0.9960\n",
      "epoch: 1100, loss = 0.9936\n",
      "epoch: 1120, loss = 0.9911\n",
      "epoch: 1140, loss = 0.9886\n",
      "epoch: 1160, loss = 0.9861\n",
      "epoch: 1180, loss = 0.9835\n",
      "epoch: 1200, loss = 0.9810\n",
      "epoch: 1220, loss = 0.9784\n",
      "epoch: 1240, loss = 0.9758\n",
      "epoch: 1260, loss = 0.9733\n",
      "epoch: 1280, loss = 0.9706\n",
      "epoch: 1300, loss = 0.9680\n",
      "epoch: 1320, loss = 0.9654\n",
      "epoch: 1340, loss = 0.9628\n",
      "epoch: 1360, loss = 0.9601\n",
      "epoch: 1380, loss = 0.9574\n",
      "epoch: 1400, loss = 0.9548\n",
      "epoch: 1420, loss = 0.9520\n",
      "epoch: 1440, loss = 0.9493\n",
      "epoch: 1460, loss = 0.9466\n",
      "epoch: 1480, loss = 0.9439\n",
      "epoch: 1500, loss = 0.9411\n",
      "epoch: 1520, loss = 0.9383\n",
      "epoch: 1540, loss = 0.9355\n",
      "epoch: 1560, loss = 0.9327\n",
      "epoch: 1580, loss = 0.9298\n",
      "epoch: 1600, loss = 0.9269\n",
      "epoch: 1620, loss = 0.9240\n",
      "epoch: 1640, loss = 0.9210\n",
      "epoch: 1660, loss = 0.9181\n",
      "epoch: 1680, loss = 0.9151\n",
      "epoch: 1700, loss = 0.9121\n",
      "epoch: 1720, loss = 0.9091\n",
      "epoch: 1740, loss = 0.9061\n",
      "epoch: 1760, loss = 0.9030\n",
      "epoch: 1780, loss = 0.8999\n",
      "epoch: 1800, loss = 0.8969\n",
      "epoch: 1820, loss = 0.8938\n",
      "epoch: 1840, loss = 0.8907\n",
      "epoch: 1860, loss = 0.8875\n",
      "epoch: 1880, loss = 0.8844\n",
      "epoch: 1900, loss = 0.8812\n",
      "epoch: 1920, loss = 0.8781\n",
      "epoch: 1940, loss = 0.8749\n",
      "epoch: 1960, loss = 0.8717\n",
      "epoch: 1980, loss = 0.8685\n",
      "epoch: 2000, loss = 0.8653\n",
      "epoch: 2020, loss = 0.8621\n",
      "epoch: 2040, loss = 0.8588\n",
      "epoch: 2060, loss = 0.8556\n",
      "epoch: 2080, loss = 0.8523\n",
      "epoch: 2100, loss = 0.8490\n",
      "epoch: 2120, loss = 0.8457\n",
      "epoch: 2140, loss = 0.8424\n",
      "epoch: 2160, loss = 0.8391\n",
      "epoch: 2180, loss = 0.8357\n",
      "epoch: 2200, loss = 0.8324\n",
      "epoch: 2220, loss = 0.8290\n",
      "epoch: 2240, loss = 0.8256\n",
      "epoch: 2260, loss = 0.8222\n",
      "epoch: 2280, loss = 0.8188\n",
      "epoch: 2300, loss = 0.8154\n",
      "epoch: 2320, loss = 0.8120\n",
      "epoch: 2340, loss = 0.8086\n",
      "epoch: 2360, loss = 0.8051\n",
      "epoch: 2380, loss = 0.8016\n",
      "epoch: 2400, loss = 0.7982\n",
      "epoch: 2420, loss = 0.7947\n",
      "epoch: 2440, loss = 0.7912\n",
      "epoch: 2460, loss = 0.7877\n",
      "epoch: 2480, loss = 0.7842\n",
      "epoch: 2500, loss = 0.7807\n",
      "epoch: 2520, loss = 0.7771\n",
      "epoch: 2540, loss = 0.7736\n",
      "epoch: 2560, loss = 0.7700\n",
      "epoch: 2580, loss = 0.7665\n",
      "epoch: 2600, loss = 0.7629\n",
      "epoch: 2620, loss = 0.7593\n",
      "epoch: 2640, loss = 0.7557\n",
      "epoch: 2660, loss = 0.7521\n",
      "epoch: 2680, loss = 0.7485\n",
      "epoch: 2700, loss = 0.7448\n",
      "epoch: 2720, loss = 0.7412\n",
      "epoch: 2740, loss = 0.7375\n",
      "epoch: 2760, loss = 0.7338\n",
      "epoch: 2780, loss = 0.7301\n",
      "epoch: 2800, loss = 0.7264\n",
      "epoch: 2820, loss = 0.7227\n",
      "epoch: 2840, loss = 0.7190\n",
      "epoch: 2860, loss = 0.7153\n",
      "epoch: 2880, loss = 0.7116\n",
      "epoch: 2900, loss = 0.7078\n",
      "epoch: 2920, loss = 0.7041\n",
      "epoch: 2940, loss = 0.7003\n",
      "epoch: 2960, loss = 0.6965\n",
      "epoch: 2980, loss = 0.6928\n",
      "epoch: 3000, loss = 0.6890\n",
      "epoch: 3020, loss = 0.6852\n",
      "epoch: 3040, loss = 0.6814\n",
      "epoch: 3060, loss = 0.6776\n",
      "epoch: 3080, loss = 0.6738\n",
      "epoch: 3100, loss = 0.6700\n",
      "epoch: 3120, loss = 0.6662\n",
      "epoch: 3140, loss = 0.6623\n",
      "epoch: 3160, loss = 0.6585\n",
      "epoch: 3180, loss = 0.6547\n",
      "epoch: 3200, loss = 0.6509\n",
      "epoch: 3220, loss = 0.6470\n",
      "epoch: 3240, loss = 0.6432\n",
      "epoch: 3260, loss = 0.6394\n",
      "epoch: 3280, loss = 0.6355\n",
      "epoch: 3300, loss = 0.6317\n",
      "epoch: 3320, loss = 0.6279\n",
      "epoch: 3340, loss = 0.6240\n",
      "epoch: 3360, loss = 0.6202\n",
      "epoch: 3380, loss = 0.6164\n",
      "epoch: 3400, loss = 0.6126\n",
      "epoch: 3420, loss = 0.6087\n",
      "epoch: 3440, loss = 0.6049\n",
      "epoch: 3460, loss = 0.6011\n",
      "epoch: 3480, loss = 0.5973\n",
      "epoch: 3500, loss = 0.5935\n",
      "epoch: 3520, loss = 0.5896\n",
      "epoch: 3540, loss = 0.5858\n",
      "epoch: 3560, loss = 0.5821\n",
      "epoch: 3580, loss = 0.5783\n",
      "epoch: 3600, loss = 0.5745\n",
      "epoch: 3620, loss = 0.5707\n",
      "epoch: 3640, loss = 0.5669\n",
      "epoch: 3660, loss = 0.5631\n",
      "epoch: 3680, loss = 0.5594\n",
      "epoch: 3700, loss = 0.5556\n",
      "epoch: 3720, loss = 0.5519\n",
      "epoch: 3740, loss = 0.5481\n",
      "epoch: 3760, loss = 0.5444\n",
      "epoch: 3780, loss = 0.5407\n",
      "epoch: 3800, loss = 0.5369\n",
      "epoch: 3820, loss = 0.5332\n",
      "epoch: 3840, loss = 0.5295\n",
      "epoch: 3860, loss = 0.5258\n",
      "epoch: 3880, loss = 0.5222\n",
      "epoch: 3900, loss = 0.5185\n",
      "epoch: 3920, loss = 0.5148\n",
      "epoch: 3940, loss = 0.5112\n",
      "epoch: 3960, loss = 0.5075\n",
      "epoch: 3980, loss = 0.5039\n",
      "epoch: 4000, loss = 0.5003\n",
      "epoch: 4020, loss = 0.4967\n",
      "epoch: 4040, loss = 0.4931\n",
      "epoch: 4060, loss = 0.4895\n",
      "epoch: 4080, loss = 0.4860\n",
      "epoch: 4100, loss = 0.4824\n",
      "epoch: 4120, loss = 0.4789\n",
      "epoch: 4140, loss = 0.4754\n",
      "epoch: 4160, loss = 0.4719\n",
      "epoch: 4180, loss = 0.4684\n",
      "epoch: 4200, loss = 0.4650\n",
      "epoch: 4220, loss = 0.4615\n",
      "epoch: 4240, loss = 0.4581\n",
      "epoch: 4260, loss = 0.4547\n",
      "epoch: 4280, loss = 0.4513\n",
      "epoch: 4300, loss = 0.4479\n",
      "epoch: 4320, loss = 0.4445\n",
      "epoch: 4340, loss = 0.4412\n",
      "epoch: 4360, loss = 0.4379\n",
      "epoch: 4380, loss = 0.4346\n",
      "epoch: 4400, loss = 0.4313\n",
      "epoch: 4420, loss = 0.4280\n",
      "epoch: 4440, loss = 0.4248\n",
      "epoch: 4460, loss = 0.4215\n",
      "epoch: 4480, loss = 0.4183\n",
      "epoch: 4500, loss = 0.4152\n",
      "epoch: 4520, loss = 0.4120\n",
      "epoch: 4540, loss = 0.4089\n",
      "epoch: 4560, loss = 0.4057\n",
      "epoch: 4580, loss = 0.4026\n",
      "epoch: 4600, loss = 0.3996\n",
      "epoch: 4620, loss = 0.3965\n",
      "epoch: 4640, loss = 0.3935\n",
      "epoch: 4660, loss = 0.3905\n",
      "epoch: 4680, loss = 0.3875\n",
      "epoch: 4700, loss = 0.3845\n",
      "epoch: 4720, loss = 0.3815\n",
      "epoch: 4740, loss = 0.3786\n",
      "epoch: 4760, loss = 0.3757\n",
      "epoch: 4780, loss = 0.3728\n",
      "epoch: 4800, loss = 0.3700\n",
      "epoch: 4820, loss = 0.3671\n",
      "epoch: 4840, loss = 0.3643\n",
      "epoch: 4860, loss = 0.3615\n",
      "epoch: 4880, loss = 0.3588\n",
      "epoch: 4900, loss = 0.3560\n",
      "epoch: 4920, loss = 0.3533\n",
      "epoch: 4940, loss = 0.3506\n",
      "epoch: 4960, loss = 0.3479\n",
      "epoch: 4980, loss = 0.3453\n",
      "epoch: 5000, loss = 0.3426\n",
      "epoch: 5020, loss = 0.3400\n",
      "epoch: 5040, loss = 0.3374\n",
      "epoch: 5060, loss = 0.3349\n",
      "epoch: 5080, loss = 0.3323\n",
      "epoch: 5100, loss = 0.3298\n",
      "epoch: 5120, loss = 0.3273\n",
      "epoch: 5140, loss = 0.3248\n",
      "epoch: 5160, loss = 0.3224\n",
      "epoch: 5180, loss = 0.3199\n",
      "epoch: 5200, loss = 0.3175\n",
      "epoch: 5220, loss = 0.3152\n",
      "epoch: 5240, loss = 0.3128\n",
      "epoch: 5260, loss = 0.3104\n",
      "epoch: 5280, loss = 0.3081\n",
      "epoch: 5300, loss = 0.3058\n",
      "epoch: 5320, loss = 0.3035\n",
      "epoch: 5340, loss = 0.3013\n",
      "epoch: 5360, loss = 0.2990\n",
      "epoch: 5380, loss = 0.2968\n",
      "epoch: 5400, loss = 0.2946\n",
      "epoch: 5420, loss = 0.2925\n",
      "epoch: 5440, loss = 0.2903\n",
      "epoch: 5460, loss = 0.2882\n",
      "epoch: 5480, loss = 0.2861\n",
      "epoch: 5500, loss = 0.2840\n",
      "epoch: 5520, loss = 0.2819\n",
      "epoch: 5540, loss = 0.2799\n",
      "epoch: 5560, loss = 0.2778\n",
      "epoch: 5580, loss = 0.2758\n",
      "epoch: 5600, loss = 0.2738\n",
      "epoch: 5620, loss = 0.2719\n",
      "epoch: 5640, loss = 0.2699\n",
      "epoch: 5660, loss = 0.2680\n",
      "epoch: 5680, loss = 0.2661\n",
      "epoch: 5700, loss = 0.2642\n",
      "epoch: 5720, loss = 0.2623\n",
      "epoch: 5740, loss = 0.2605\n",
      "epoch: 5760, loss = 0.2586\n",
      "epoch: 5780, loss = 0.2568\n",
      "epoch: 5800, loss = 0.2550\n",
      "epoch: 5820, loss = 0.2533\n",
      "epoch: 5840, loss = 0.2515\n",
      "epoch: 5860, loss = 0.2498\n",
      "epoch: 5880, loss = 0.2480\n",
      "epoch: 5900, loss = 0.2463\n",
      "epoch: 5920, loss = 0.2447\n",
      "epoch: 5940, loss = 0.2430\n",
      "epoch: 5960, loss = 0.2413\n",
      "epoch: 5980, loss = 0.2397\n",
      "epoch: 6000, loss = 0.2381\n",
      "epoch: 6020, loss = 0.2365\n",
      "epoch: 6040, loss = 0.2349\n",
      "epoch: 6060, loss = 0.2333\n",
      "epoch: 6080, loss = 0.2318\n",
      "epoch: 6100, loss = 0.2303\n",
      "epoch: 6120, loss = 0.2287\n",
      "epoch: 6140, loss = 0.2272\n",
      "epoch: 6160, loss = 0.2257\n",
      "epoch: 6180, loss = 0.2243\n",
      "epoch: 6200, loss = 0.2228\n",
      "epoch: 6220, loss = 0.2214\n",
      "epoch: 6240, loss = 0.2200\n",
      "epoch: 6260, loss = 0.2185\n",
      "epoch: 6280, loss = 0.2171\n",
      "epoch: 6300, loss = 0.2158\n",
      "epoch: 6320, loss = 0.2144\n",
      "epoch: 6340, loss = 0.2130\n",
      "epoch: 6360, loss = 0.2117\n",
      "epoch: 6380, loss = 0.2104\n",
      "epoch: 6400, loss = 0.2091\n",
      "epoch: 6420, loss = 0.2078\n",
      "epoch: 6440, loss = 0.2065\n",
      "epoch: 6460, loss = 0.2052\n",
      "epoch: 6480, loss = 0.2040\n",
      "epoch: 6500, loss = 0.2027\n",
      "epoch: 6520, loss = 0.2015\n",
      "epoch: 6540, loss = 0.2003\n",
      "epoch: 6560, loss = 0.1991\n",
      "epoch: 6580, loss = 0.1979\n",
      "epoch: 6600, loss = 0.1967\n",
      "epoch: 6620, loss = 0.1955\n",
      "epoch: 6640, loss = 0.1944\n",
      "epoch: 6660, loss = 0.1932\n",
      "epoch: 6680, loss = 0.1921\n",
      "epoch: 6700, loss = 0.1910\n",
      "epoch: 6720, loss = 0.1899\n",
      "epoch: 6740, loss = 0.1888\n",
      "epoch: 6760, loss = 0.1877\n",
      "epoch: 6780, loss = 0.1866\n",
      "epoch: 6800, loss = 0.1856\n",
      "epoch: 6820, loss = 0.1845\n",
      "epoch: 6840, loss = 0.1835\n",
      "epoch: 6860, loss = 0.1824\n",
      "epoch: 6880, loss = 0.1814\n",
      "epoch: 6900, loss = 0.1804\n",
      "epoch: 6920, loss = 0.1794\n",
      "epoch: 6940, loss = 0.1784\n",
      "epoch: 6960, loss = 0.1774\n",
      "epoch: 6980, loss = 0.1765\n",
      "epoch: 7000, loss = 0.1755\n",
      "epoch: 7020, loss = 0.1746\n",
      "epoch: 7040, loss = 0.1736\n",
      "epoch: 7060, loss = 0.1727\n",
      "epoch: 7080, loss = 0.1718\n",
      "epoch: 7100, loss = 0.1709\n",
      "epoch: 7120, loss = 0.1700\n",
      "epoch: 7140, loss = 0.1691\n",
      "epoch: 7160, loss = 0.1682\n",
      "epoch: 7180, loss = 0.1673\n",
      "epoch: 7200, loss = 0.1664\n",
      "epoch: 7220, loss = 0.1656\n",
      "epoch: 7240, loss = 0.1647\n",
      "epoch: 7260, loss = 0.1639\n",
      "epoch: 7280, loss = 0.1631\n",
      "epoch: 7300, loss = 0.1622\n",
      "epoch: 7320, loss = 0.1614\n",
      "epoch: 7340, loss = 0.1606\n",
      "epoch: 7360, loss = 0.1598\n",
      "epoch: 7380, loss = 0.1590\n",
      "epoch: 7400, loss = 0.1582\n",
      "epoch: 7420, loss = 0.1574\n",
      "epoch: 7440, loss = 0.1567\n",
      "epoch: 7460, loss = 0.1559\n",
      "epoch: 7480, loss = 0.1551\n",
      "epoch: 7500, loss = 0.1544\n",
      "epoch: 7520, loss = 0.1537\n",
      "epoch: 7540, loss = 0.1529\n",
      "epoch: 7560, loss = 0.1522\n",
      "epoch: 7580, loss = 0.1515\n",
      "epoch: 7600, loss = 0.1508\n",
      "epoch: 7620, loss = 0.1500\n",
      "epoch: 7640, loss = 0.1493\n",
      "epoch: 7660, loss = 0.1486\n",
      "epoch: 7680, loss = 0.1480\n",
      "epoch: 7700, loss = 0.1473\n",
      "epoch: 7720, loss = 0.1466\n",
      "epoch: 7740, loss = 0.1459\n",
      "epoch: 7760, loss = 0.1453\n",
      "epoch: 7780, loss = 0.1446\n",
      "epoch: 7800, loss = 0.1440\n",
      "epoch: 7820, loss = 0.1433\n",
      "epoch: 7840, loss = 0.1427\n",
      "epoch: 7860, loss = 0.1420\n",
      "epoch: 7880, loss = 0.1414\n",
      "epoch: 7900, loss = 0.1408\n",
      "epoch: 7920, loss = 0.1402\n",
      "epoch: 7940, loss = 0.1396\n",
      "epoch: 7960, loss = 0.1390\n",
      "epoch: 7980, loss = 0.1384\n",
      "epoch: 8000, loss = 0.1378\n",
      "epoch: 8020, loss = 0.1372\n",
      "epoch: 8040, loss = 0.1366\n",
      "epoch: 8060, loss = 0.1360\n",
      "epoch: 8080, loss = 0.1354\n",
      "epoch: 8100, loss = 0.1349\n",
      "epoch: 8120, loss = 0.1343\n",
      "epoch: 8140, loss = 0.1337\n",
      "epoch: 8160, loss = 0.1332\n",
      "epoch: 8180, loss = 0.1326\n",
      "epoch: 8200, loss = 0.1321\n",
      "epoch: 8220, loss = 0.1315\n",
      "epoch: 8240, loss = 0.1310\n",
      "epoch: 8260, loss = 0.1305\n",
      "epoch: 8280, loss = 0.1299\n",
      "epoch: 8300, loss = 0.1294\n",
      "epoch: 8320, loss = 0.1289\n",
      "epoch: 8340, loss = 0.1284\n",
      "epoch: 8360, loss = 0.1279\n",
      "epoch: 8380, loss = 0.1274\n",
      "epoch: 8400, loss = 0.1269\n",
      "epoch: 8420, loss = 0.1264\n",
      "epoch: 8440, loss = 0.1259\n",
      "epoch: 8460, loss = 0.1254\n",
      "epoch: 8480, loss = 0.1249\n",
      "epoch: 8500, loss = 0.1244\n",
      "epoch: 8520, loss = 0.1240\n",
      "epoch: 8540, loss = 0.1235\n",
      "epoch: 8560, loss = 0.1230\n",
      "epoch: 8580, loss = 0.1225\n",
      "epoch: 8600, loss = 0.1221\n",
      "epoch: 8620, loss = 0.1216\n",
      "epoch: 8640, loss = 0.1212\n",
      "epoch: 8660, loss = 0.1207\n",
      "epoch: 8680, loss = 0.1203\n",
      "epoch: 8700, loss = 0.1198\n",
      "epoch: 8720, loss = 0.1194\n",
      "epoch: 8740, loss = 0.1190\n",
      "epoch: 8760, loss = 0.1185\n",
      "epoch: 8780, loss = 0.1181\n",
      "epoch: 8800, loss = 0.1177\n",
      "epoch: 8820, loss = 0.1172\n",
      "epoch: 8840, loss = 0.1168\n",
      "epoch: 8860, loss = 0.1164\n",
      "epoch: 8880, loss = 0.1160\n",
      "epoch: 8900, loss = 0.1156\n",
      "epoch: 8920, loss = 0.1152\n",
      "epoch: 8940, loss = 0.1148\n",
      "epoch: 8960, loss = 0.1144\n",
      "epoch: 8980, loss = 0.1140\n",
      "epoch: 9000, loss = 0.1136\n",
      "epoch: 9020, loss = 0.1132\n",
      "epoch: 9040, loss = 0.1128\n",
      "epoch: 9060, loss = 0.1124\n",
      "epoch: 9080, loss = 0.1120\n",
      "epoch: 9100, loss = 0.1116\n",
      "epoch: 9120, loss = 0.1113\n",
      "epoch: 9140, loss = 0.1109\n",
      "epoch: 9160, loss = 0.1105\n",
      "epoch: 9180, loss = 0.1102\n",
      "epoch: 9200, loss = 0.1098\n",
      "epoch: 9220, loss = 0.1094\n",
      "epoch: 9240, loss = 0.1091\n",
      "epoch: 9260, loss = 0.1087\n",
      "epoch: 9280, loss = 0.1084\n",
      "epoch: 9300, loss = 0.1080\n",
      "epoch: 9320, loss = 0.1076\n",
      "epoch: 9340, loss = 0.1073\n",
      "epoch: 9360, loss = 0.1070\n",
      "epoch: 9380, loss = 0.1066\n",
      "epoch: 9400, loss = 0.1063\n",
      "epoch: 9420, loss = 0.1059\n",
      "epoch: 9440, loss = 0.1056\n",
      "epoch: 9460, loss = 0.1053\n",
      "epoch: 9480, loss = 0.1049\n",
      "epoch: 9500, loss = 0.1046\n",
      "epoch: 9520, loss = 0.1043\n",
      "epoch: 9540, loss = 0.1040\n",
      "epoch: 9560, loss = 0.1036\n",
      "epoch: 9580, loss = 0.1033\n",
      "epoch: 9600, loss = 0.1030\n",
      "epoch: 9620, loss = 0.1027\n",
      "epoch: 9640, loss = 0.1024\n",
      "epoch: 9660, loss = 0.1021\n",
      "epoch: 9680, loss = 0.1017\n",
      "epoch: 9700, loss = 0.1014\n",
      "epoch: 9720, loss = 0.1011\n",
      "epoch: 9740, loss = 0.1008\n",
      "epoch: 9760, loss = 0.1005\n",
      "epoch: 9780, loss = 0.1002\n",
      "epoch: 9800, loss = 0.0999\n",
      "epoch: 9820, loss = 0.0996\n",
      "epoch: 9840, loss = 0.0993\n",
      "epoch: 9860, loss = 0.0990\n",
      "epoch: 9880, loss = 0.0988\n",
      "epoch: 9900, loss = 0.0985\n",
      "epoch: 9920, loss = 0.0982\n",
      "epoch: 9940, loss = 0.0979\n",
      "epoch: 9960, loss = 0.0976\n",
      "epoch: 9980, loss = 0.0973\n",
      "epoch: 10000, loss = 0.0971\n"
     ]
    }
   ],
   "source": [
    "#Train the Model\n",
    "# ============================ step 5/6 training ============================    \n",
    "for epoch in range(num_epochs):\n",
    "    models.train()\n",
    "    optimizers.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred  = models(X_train)\n",
    "    # Compute Loss\n",
    "    loss = criterions(y_pred, y_train)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizers.step()\n",
    "    if (epoch+1) % 20 == 0:                                         \n",
    "        # printing loss values on every 10 epochs to keep track\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \n",
    "+ when you call `models(X_train)`, you automatically call `models.forward()` to propagate forward.\n",
    "+  Next, the loss is calculated. When `loss.backward()` is called, it computes the loss gradient with respect to the weights (of the layer). \n",
    "+ The weights are then updated by calling `optimizer.step()`. \n",
    "+ After this, the weights have to be emptied for the next iteration. So the `zero_grad()` method is called.\n",
    "\n",
    "The above code prints the loss at each 20th epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 7 Model Performance\n",
    "  \n",
    "Let us finally see the model accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = models(X_test)\n",
    "    y_pred = nn.Softmax(dim=1)(logits)\n",
    "    y_predicted_cls = y_pred.argmax(1)\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.96      0.96      0.96        20\n",
      "weighted avg       0.96      0.95      0.95        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "### Exercise 1 logistic regression (50 points )\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  Handwriting recognition with MLP(50 points )\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "<font color='red' size=4>Note that your accuracy in this section will directly determine your score.</font>\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3  Questions (10 points )\n",
    "1.What's the difference between logistic regression and Perceptron?\n",
    "\n",
    "2.Advantages and disadvantages of neural networks?\n",
    "\n",
    "3.What is the role of Activation Function in Neural networks?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cf8428aa180ee23632ed7df20f7a595edda7c60e668686876baf89d702ea1cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
